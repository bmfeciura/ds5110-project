{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType,  StringType\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./jordan_subset\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+---+---------+----------+------------+----------------------+--------------------+-------------+-------+--------------+------+-----+--------+------------------+-----+------------+--------------------+-------------+------+----------------+----------+\n",
      "|subreddit_id|    _c0|created_utc|ups|  link_id|      name|score_hidden|author_flair_css_class|   author_flair_text|    subreddit|     id|removal_reason|gilded|downs|archived|            author|score|retrieved_on|                body|distinguished|edited|controversiality| parent_id|\n",
      "+------------+-------+-----------+---+---------+----------+------------+----------------------+--------------------+-------------+-------+--------------+------+-----+--------+------------------+-----+------------+--------------------+-------------+------+----------------+----------+\n",
      "|    t5_2qh33|5027245| 1430702078|  1|t3_34rftl|t1_cqxhekl|           0|                    NA|                  NA|        funny|cqxhekl|            NA|     0|    0|       0|         kimuchi82|    1|  1432755320|Is it another fuc...|           NA|     0|               0| t3_34rftl|\n",
      "|    t5_2qm7u|5027247| 1430702078|  1|t3_34r584|t1_cqxhekn|           0|                   nyy|    New York Yankees|     baseball|cqxhekn|            NA|     0|    0|       0|pinata_penis_pump2|    1|  1432755320|First year as a s...|           NA|     0|               0|t1_cqxhcj3|\n",
      "|    t5_2rjz2|5027248| 1430702078|  2|t3_34rvno|t1_cqxheko|           0|                    NA|                  NA|gameofthrones|cqxheko|            NA|     0|    0|       0|         [deleted]|    2|  1432755320|           [deleted]|           NA|     0|               0| t3_34rvno|\n",
      "|    t5_2qh1e|5027249| 1430702078|  3|t3_34qhni|t1_cqxhekp|           0|                    NA|                  NA|       videos|cqxhekp|            NA|     0|    0|       0|          unclened|    3|  1432755320|I've read and see...|           NA|     0|               0|t1_cqxdxyl|\n",
      "|    t5_2r2o9|5027255| 1430702078|  3|t3_34rue8|t1_cqxhekv|           0|           NightsWatch|There might be so...|       asoiaf|cqxhekv|            NA|     0|    0|       0|       Redwinevino|    3|  1432755320|\"High Septon: \"\"I...|         null|  null|            null|      null|\n",
      "+------------+-------+-----------+---+---------+----------+------------+----------------------+--------------------+-------------+-------+--------------+------+-----+--------+------------------+-----+------------+--------------------+-------------+------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4871463"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subreddit_id',\n",
       " '_c0',\n",
       " 'created_utc',\n",
       " 'ups',\n",
       " 'link_id',\n",
       " 'name',\n",
       " 'score_hidden',\n",
       " 'author_flair_css_class',\n",
       " 'author_flair_text',\n",
       " 'subreddit',\n",
       " 'id',\n",
       " 'removal_reason',\n",
       " 'gilded',\n",
       " 'downs',\n",
       " 'archived',\n",
       " 'author',\n",
       " 'score',\n",
       " 'retrieved_on',\n",
       " 'body',\n",
       " 'distinguished',\n",
       " 'edited',\n",
       " 'controversiality',\n",
       " 'parent_id']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('subreddit_id', 'string'),\n",
       " ('ups', 'string'),\n",
       " ('gilded', 'string'),\n",
       " ('score_hidden', 'string'),\n",
       " ('downs', 'string'),\n",
       " ('score', 'string'),\n",
       " ('controversiality', 'string'),\n",
       " ('body', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.select(['subreddit_id','ups','gilded','score_hidden','downs','score','controversiality','body'])\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('subreddit_id', 'string'),\n",
       " ('ups', 'int'),\n",
       " ('gilded', 'int'),\n",
       " ('score_hidden', 'boolean'),\n",
       " ('downs', 'int'),\n",
       " ('score', 'int'),\n",
       " ('controversiality', 'int'),\n",
       " ('body', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType,BooleanType\n",
    "# https://stackoverflow.com/questions/46956026/how-to-convert-column-with-string-type-to-int-form-in-pyspark-data-frame\n",
    "# data_df = data_df.withColumn(\"Plays\", data_df[\"Plays\"].cast(IntegerType()))\n",
    "df2 = df2.withColumn('ups',df2['ups'].cast(IntegerType()))\n",
    "df2 = df2.withColumn('downs',df2['downs'].cast(IntegerType()))\n",
    "df2 = df2.withColumn('score',df2['score'].cast(IntegerType()))\n",
    "df2 = df2.withColumn('controversiality',df2['controversiality'].cast(IntegerType()))\n",
    "df2 = df2.withColumn('gilded',df2['gilded'].cast(IntegerType()))\n",
    "df2 = df2.withColumn('score_hidden',df2['score_hidden'].cast(BooleanType()))\n",
    "df2 = df2.fillna(0)\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|score_hidden|\n",
      "+------------+\n",
      "|        true|\n",
      "|       false|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(['score_hidden']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to training and test\n",
    "(trainingData, testData) = df2.sample(.01).randomSplit([0.3, 0.7])\n",
    "# (trainingData, testData) = df2.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14473"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34010"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "# tk_data = tk.transform(trainingData)\n",
    "# tk_data.select('body','words').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "# sw_data = sw.transform(tk_data)\n",
    "# sw_data.select('body','words','filtered').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"counted\", vocabSize=3, minDF=2.0)\n",
    "# cv_fit = cv.fit(temp)\n",
    "# cv_data = cv_fit.transform(temp)\n",
    "# cv_data.select('body','words','filtered','counted').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v =  Word2Vec(vectorSize=3, minCount=0, inputCol=\"filtered\", outputCol=\"word2vec\")\n",
    "# w2v_fit = w2v.fit(cv_data)\n",
    "# w2v_data = w2v_fit.transform(cv_data)\n",
    "# w2v_data.select('body','words','filtered',\"word2vec\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = StringIndexer(inputCol=\"subreddit_id\", outputCol=\"sr_id_num\") # maybe I don't need this.\n",
    "# si_model = si.fit(w2v_data)\n",
    "# si_data = si_model.transform(w2v_data)\n",
    "\n",
    "# si_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats =  ['ups','gilded','score_hidden','downs','score','controversiality','counted','word2vec']\n",
    "feats =  ['ups','gilded','score_hidden','downs','score','controversiality','word2vec']\n",
    "assembler = VectorAssembler(inputCols=feats, outputCol=\"features\")\n",
    "# assembler_data = assembler.transform(si_data)\n",
    "\n",
    "# assembler_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol='sr_id_num', featuresCol='features')\n",
    "# rf_model = rf.fit(assembler_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline(stages=[tk,sw,cv,w2v,si,assembler,rf])\n",
    "pipeline = Pipeline(stages=[tk,sw,w2v,si,assembler,rf])\n",
    "model = pipeline.fit(trainingData)\n",
    "prediction = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"sr_id_num\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# evaluator = MulticlassClassificationEvaluator(\n",
    "#     labelCol=“n_index”, predictionCol=“prediction”, metricName=“accuracy”)\n",
    "# accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.linalg import Vectors\n",
    "# from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# transformed_df = df.rdd.map(lambda row: LabeledPoint(row[0], Vectors.dense(row[0:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/32556178/create-labeledpoints-from-spark-dataframe-in-python\n",
    "# (vec.select(col(\"outcome_column\").alias(\"label\"), col(\"features\"))\n",
    "#   .rdd\n",
    "#   .map(lambda row: LabeledPoint(row.label, row.features)))\n",
    "\n",
    "# w2v_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe = OneHotEncoder(inputCol=\"sr_id_num\", outputCol=\"subr_ohe\")  \n",
    "# ohe_fit = ohe.fit(si_data)\n",
    "# ohe_data = ohe_fit.transform(si_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.  #david\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "# model = RandomForest.trainClassifier(vec_rdd, numClasses=1000, categoricalFeaturesInfo={1:2,2:2},\n",
    "#                                      numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "#                                      impurity='gini', maxDepth=3, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_rdd = vec.rdd.map(tuple).take(5)\n",
    "# features = vec.select(feats).rdd.map(tuple)\n",
    "# rdd = vec.select(['subreddit_id','ups','gilded','score_hidden','downs','score','controversiality','counted','word2vec']).rdd.map(lambda x: LabeledPoint(x[0],x[1:]))\n",
    "# LabeledPoint(label, features)\n",
    "\n",
    "\n",
    "# def parsePoint(line):\n",
    "#     values = [float(x) for x in line.split(' ')]\n",
    "#     return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "# parsedData = data.map(parsePoint)\n",
    "\n",
    "# model_data = si_data.select(['ups','gilded','score_hidden','downs','score','controversiality','counted','word2vec','sr_id_num'])\n",
    "# model_data = si_data.select(['ups','gilded','score_hidden','downs','score','controversiality','sr_id_num'])\n",
    "# rdd = model_data.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "\n",
    "# model_data.dtypes\n",
    "# model_data.show(5)\n",
    "# rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
