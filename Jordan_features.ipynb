{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType,  StringType\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./jordan_subset\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+---+---------+----------+------------+----------------------+-----------------+---------------+-------+--------------+------+-----+--------+---------------+-----+------------+--------------------+-------------+------+----------------+----------+\n",
      "|subreddit_id|    _c0|created_utc|ups|  link_id|      name|score_hidden|author_flair_css_class|author_flair_text|      subreddit|     id|removal_reason|gilded|downs|archived|         author|score|retrieved_on|                body|distinguished|edited|controversiality| parent_id|\n",
      "+------------+-------+-----------+---+---------+----------+------------+----------------------+-----------------+---------------+-------+--------------+------+-----+--------+---------------+-----+------------+--------------------+-------------+------+----------------+----------+\n",
      "|    t5_2te6p|3345542| 1430609172|  4|t3_34mhlr|t1_cqwgwou|           0|                    NA|               NA|WaltDisneyWorld|cqwgwou|            NA|     0|    0|       0|    orangekid13|    4|  1432737889|Those helicopters...|           NA|     0|               0|t1_cqw6yd9|\n",
      "|    t5_2qh55|3345543| 1430609172|  1|t3_34m80e|t1_cqwgwov|           0|                    NA|               NA|           food|cqwgwov|            NA|     0|    0|       0|   colintheking|    1|  1432737889|     OMG.  Mouthgasm|           NA|     0|               0| t3_34m80e|\n",
      "|    t5_2rfxx|3345544| 1430609172|  2|t3_34lxnr|t1_cqwgwow|           0|              katarina|             null|leagueoflegends|cqwgwow|            NA|     0|    0|       0|   aldothetroll|    2|  1432737889|Jinx is such free...|         null|  null|            null|      null|\n",
      "|    t5_2qlqh|3345545| 1430609172|  2|t3_34m2d1|t1_cqwgwox|           0|                    NA|               NA|        Android|cqwgwox|            NA|     0|    0|       0|Kittycat-banana|    2|  1432737889|That worked! Than...|           NA|     0|               0|t1_cqw5tch|\n",
      "|    t5_2qh3p|3345547| 1430609172|  1|t3_34mzrt|t1_cqwgwoz|           0|                    NA|               NA|            sex|cqwgwoz|            NA|     0|    0|       0|   thatoneguy54|    1|  1432737889|Huh, TIL. How exa...|           NA|     0|               0|t1_cqwgtv6|\n",
      "+------------+-------+-----------+---+---------+----------+------------+----------------------+-----------------+---------------+-------+--------------+------+-----+--------+---------------+-----+------------+--------------------+-------------+------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8799960"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subreddit_id',\n",
       " '_c0',\n",
       " 'created_utc',\n",
       " 'ups',\n",
       " 'link_id',\n",
       " 'name',\n",
       " 'score_hidden',\n",
       " 'author_flair_css_class',\n",
       " 'author_flair_text',\n",
       " 'subreddit',\n",
       " 'id',\n",
       " 'removal_reason',\n",
       " 'gilded',\n",
       " 'downs',\n",
       " 'archived',\n",
       " 'author',\n",
       " 'score',\n",
       " 'retrieved_on',\n",
       " 'body',\n",
       " 'distinguished',\n",
       " 'edited',\n",
       " 'controversiality',\n",
       " 'parent_id']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "# then stop words remover\n",
    "# then countvectorizer\n",
    "# word2vec??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "# tokenized = tk.transform(df)\n",
    "# tokenized.select('body','words').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "# removedData = sw.transform(tokenized)\n",
    "# removedData.select('body','words','filtered').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"counted\", vocabSize=3, minDF=2.0)\n",
    "# counted = cv.fit(removedData)\n",
    "# counted.select('body','words','filtered','counted').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v =  Word2Vec(vectorSize=3, minCount=0, inputCol=\"filtered\", outputCol=\"word2vec\")\n",
    "# vectored = w2v.fit(removedData)\n",
    "# vectored.select('body','words','filtered',\"word2vec\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = StringIndexer(inputCol=\"subreddit_id\", outputCol=\"sr_id_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(inputCol=\"sr_id_num\", outputCol=\"subr_ohe\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats =  ['ups','gilded','score_hidden','downs','score','controversiality','counted','word2vec']\n",
    "feats =  ['ups','gilded','score_hidden','downs','score','controversiality']\n",
    "vecAs = VectorAssembler(inputCols=feats, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this all to a pipeline before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline(stages=[tk,sw,cv,w2v,ohe,vecAs])\n",
    "pipeline = Pipeline(stages=[tk,sw,si,ohe,vecAs])\n",
    "df_fitted = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we do an unsupervised analysis and pass in subreddits? Is there a supervised method we can use?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
