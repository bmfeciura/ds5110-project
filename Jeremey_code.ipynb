{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 5110 Group Project\n",
    "Team: Alexandra Cathcart (adc6fs), Benjamin Feciura (bmf3bw), Jeremey Donovan (jdd5dw), Jordan Hiatt (jdh2e)\n",
    "\n",
    "Original data: https://www.kaggle.com/reddit/reddit-comments-may-2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Includes & Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col, countDistinct, lower, size, split, udf, when\n",
    "from pyspark.sql.types import ArrayType, FloatType, IntegerType,  StringType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA is a little slow so runEDA = 1 to run\n",
    "runEDA=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, holdout\n",
    "# key code control b/c we have been experiencing memory issues with a 50/50 train/test split\n",
    "trainPct=0.1\n",
    "testPct=0.1\n",
    "holdoutPct=0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persisting trainDF should speed training but we have been experiencing memory issues\n",
    "persistTrainDF=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-ride parallelism: We have been expreiencing memory issues. Set to anything other than 0 to override.\n",
    "# Otherwise, set to the desired over-ride integer value\n",
    "overrideParallelism=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadCVmodel: If blank, then do not load and instead run CV\n",
    "# otherwise, provide name of cv model to load\n",
    "#loadCVmodel=\"\"\n",
    "loadCVmodel=\"lrModel20210730-005848\"\n",
    "\n",
    "loadCVmodelSens=\"lrModelSens20210730-093722\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IMPLEMENTED!!!\n",
    "\"\"\"\n",
    "# loadRFmodel: If blank, then do not load and instead run RF baseline model\n",
    "# otherwise, provide name of RF model to load\n",
    "#loadCVmodel=\"\"\n",
    "loadRFmodel=\"rfModel20210730-211517\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the reddit data\n",
    "full_path = '/project/ds5559/r-slash-group8/sample.csv'\n",
    "\n",
    "df = spark.read.csv(full_path,  inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Bad Word data\n",
    "schema = StructType().add(\"badWord\",StringType(),True)\n",
    "dfBW=spark.read.format(\"csv\").schema(schema).load('bad_words.csv')\n",
    "#  dfBW.show(5)  # not showing since words are quite vulgar\n",
    "\n",
    "# Also create in list format\n",
    "listBW=list(dfBW.select('badWord').toPandas()['badWord']) \n",
    "# listBW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regex with all the bad words\n",
    "# if there is an issue, try \\\\\\\\b instead; just \\b probably has issues\n",
    "listBW=list(map(lambda line: \"\\\\b\" + line + \"\\\\b\",listBW))\n",
    "delim='|'\n",
    "strBW=delim.join(listBW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ups: integer (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- removal_reason: string (nullable = true)\n",
      " |-- downs: integer (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      "\n",
      "+----+---------+--------------+-----+------------+--------------------+-------------+\n",
      "| ups|subreddit|removal_reason|downs|      author|                body|distinguished|\n",
      "+----+---------+--------------+-----+------------+--------------------+-------------+\n",
      "|   4|soccer_jp|            NA|    0|       rx109|                くそ|         null|\n",
      "|null|     null|          null| null|        null|                null|         null|\n",
      "|   0|     null|          null| null|        null|                null|         null|\n",
      "|   4|      nba|            NA|    0|   WyaOfWade|gg this one's ove...|           NA|\n",
      "|   0| politics|            NA|    0|Wicked_Truth|Are you really im...|           NA|\n",
      "+----+---------+--------------+-----+------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop unneeded cols from dataframe\n",
    "df=df.drop('_c0','created_utc','subreddit_id','link_id','name','score_hidden','author_flair_css_class', 'gilded', \\\n",
    "        'author_flair_text','id','archived','retrieved_on', 'edited','controversiality','parent_id','score')\n",
    "\n",
    "# convert integer cols (ups, downs, and gilded) to integers\n",
    "# Note: we could have done this by defining a schema before the csv read\n",
    "df=df.withColumn(\"ups\",df.ups.cast(IntegerType()))\n",
    "df=df.withColumn(\"downs\",df.downs.cast(IntegerType()))\n",
    "#df=df.withColumn(\"gilded\",df.gilded.cast(IntegerType()))  # Removed gilded since not used in this analysis\n",
    "\n",
    "# Confirm new schema\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15317725"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows before removing NA\n",
    "df.count()\n",
    "# There are 15,317,725 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+-----+--------------+--------------------+-------------+\n",
      "|ups|subreddit|removal_reason|downs|        author|                body|distinguished|\n",
      "+---+---------+--------------+-----+--------------+--------------------+-------------+\n",
      "|  4|soccer_jp|            NA|    0|         rx109|                くそ|         null|\n",
      "|  4|      nba|            NA|    0|     WyaOfWade|gg this one's ove...|           NA|\n",
      "|  0| politics|            NA|    0|  Wicked_Truth|Are you really im...|           NA|\n",
      "|  3|AskReddit|            NA|    0|      jesse9o3|No one has a Euro...|           NA|\n",
      "|  3|AskReddit|            NA|    0|beltfedshooter|\"That the kid \"\"....|           NA|\n",
      "+---+---------+--------------+-----+--------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where up, down, or body is null. We  do this since inference of these values is not applicable\n",
    "df=df.filter(df['ups'].isNotNull())\n",
    "df=df.filter(df['downs'].isNotNull())\n",
    "df=df.filter(df['body'].isNotNull())\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the author was '[deleted]' \n",
    "df=df.filter(df['author']!='[deleted]')\n",
    "\n",
    "# Remove author \"0\"\n",
    "df=df.filter(df['author']!='0')\n",
    "\n",
    "\n",
    "# Remove rows where the author was 'AutoModerator'\n",
    "# see https://www.reddit.com/wiki/automoderator\n",
    "df=df.filter(df['author']!='AutoModerator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9226090"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows AFTER removing NA\n",
    "df.count()\n",
    "# There now 9,226,090 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all body text\n",
    "df=df.withColumn('body',lower(col('body')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+-----+--------------+--------------------+-------------+-----+\n",
      "|ups|subreddit|removal_reason|downs|        author|                body|distinguished|score|\n",
      "+---+---------+--------------+-----+--------------+--------------------+-------------+-----+\n",
      "|  4|soccer_jp|            NA|    0|         rx109|                くそ|         null|    4|\n",
      "|  4|      nba|            NA|    0|     WyaOfWade|gg this one's ove...|           NA|    4|\n",
      "|  0| politics|            NA|    0|  Wicked_Truth|are you really im...|           NA|    0|\n",
      "|  3|AskReddit|            NA|    0|      jesse9o3|no one has a euro...|           NA|    3|\n",
      "|  3|AskReddit|            NA|    0|beltfedshooter|\"that the kid \"\"....|           NA|    3|\n",
      "+---+---------+--------------+-----+--------------+--------------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Even though we dropped the column, adding score back into dataframe by computing it\n",
    "df=df.withColumn('score',df['ups']-df['downs'])\n",
    "df=df.withColumn(\"score\",df.score.cast(IntegerType()))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+-----+---------+--------------------+-------------+-----+--------------+\n",
      "|ups|subreddit|removal_reason|downs|   author|                body|distinguished|score|scoreSentiment|\n",
      "+---+---------+--------------+-----+---------+--------------------+-------------+-----+--------------+\n",
      "|  4|soccer_jp|            NA|    0|    rx109|                くそ|         null|    4|           2.0|\n",
      "|  4|      nba|            NA|    0|WyaOfWade|gg this one's ove...|           NA|    4|           2.0|\n",
      "+---+---------+--------------+-----+---------+--------------------+-------------+-----+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine a scoreSentiment as either postive, neutral, or negative.\n",
    "# This will be our response variable\n",
    "\n",
    "# Drop scoreSentiment if it already exists\n",
    "df=df.drop('scoreSentiment')\n",
    "\n",
    "# Set up bucketizer\n",
    "splits = [-float(\"inf\"), -0.1,0.1, float(\"inf\")]\n",
    "bkt = Bucketizer(splits=splits, inputCol=\"score\", outputCol=\"scoreSentiment\")\n",
    "\n",
    "# Transform to add scoreSentiment: 0=negative; 1=neutral; 2=positive.\n",
    "df=bkt.transform(df)\n",
    "\n",
    "# !!! Cannot shift to -1,0,1 since LR must start with 0 !!!\n",
    "# To make things more clear, shift to -1=negative; 0=neutral; 1=positive\n",
    "#df=df.withColumn(\"scoreSentiment\", \\\n",
    "#                 when(df['scoreSentiment']==0,-1) \\\n",
    "#                 .when(df['scoreSentiment']==1,0) \\\n",
    "#                 .otherwise(1)\n",
    "#                ) \n",
    "\n",
    "df.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag comments containing bad words\n",
    "df=df.withColumn('bwFlag',col('body').rlike(strBW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append bodyWordCount\n",
    "df=df.withColumn(\"bodyWordCount\", size(split(df['body'], ' ')))\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though not the cleanest thing to do from a data sci perspective, we\n",
    "# are going to drop the neutral sentiment rows so we can do binomial\n",
    "# rather than multinomial regression; neutral currently \"1\"\n",
    "df=df.filter(df['scoreSentiment']!=1)\n",
    "# Shift positive from 2 to 1\n",
    "df=df.withColumn(\"scoreSentiment\", \\\n",
    "                 when(df['scoreSentiment']==2,1) \\\n",
    "                 .when(df['scoreSentiment']==0,0) \\\n",
    "                 .otherwise(-1)\n",
    "                ) \n",
    "# we should never have the otherwise case!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validator explicity wants response to be called \"label\"\n",
    "# so copying scoreSentiment to label in all DFs\n",
    "df=df.withColumn(\"label\", df[\"scoreSentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=314\n",
    "trainDF,testDF, holdoutDF=df.randomSplit([trainPct,testPct,holdoutPct],seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|bwFlag|count(bwFlag)|\n",
      "+------+-------------+\n",
      "|  true|       392771|\n",
      "| false|      8433257|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if runEDA:\n",
    "    # How many comments have bad words?\n",
    "    # Confirm the flagging worked by looking at how many comments contain bad words vs good\n",
    "    # NOTE: This has a rather long runtime!!!\n",
    "    df.groupby('bwFlag').agg({\"bwFlag\":\"count\"}).show()\n",
    "    #df.filter(df['bwFlag']==True).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT author)|\n",
      "+----------------------+\n",
      "|               1216598|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if runEDA:\n",
    "    # How many authors are there?\n",
    "    df.select(countDistinct('author')).show()\n",
    "    # There are 1,216,598 authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------+--------+\n",
      "|             author|sum(score)|sum(downs)|count(author)|sum(ups)|\n",
      "+-------------------+----------+----------+-------------+--------+\n",
      "|      TheNitromeFan|     10445|         0|         3997|   10445|\n",
      "|        TweetPoster|      7090|         0|         3452|    7090|\n",
      "|        autowikibot|      6420|         0|         3188|    6420|\n",
      "|         PoliticBot|      3159|         0|         3138|    3159|\n",
      "|TweetsInCommentsBot|      9965|         0|         2999|    9965|\n",
      "|     atomicimploder|      7363|         0|         2616|    7363|\n",
      "|       Removedpixel|      5333|         0|         2264|    5333|\n",
      "|          TrollaBot|      2640|         0|         2212|    2640|\n",
      "|          havoc_bot|      2120|         0|         2101|    2120|\n",
      "|     MTGCardFetcher|      3089|         0|         2053|    3089|\n",
      "+-------------------+----------+----------+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if runEDA:\n",
    "    # Show the top 10 authors with sum of ups and downs\n",
    "    df.groupby('author').agg({\"author\":\"count\",\"ups\":\"sum\",\"downs\":\"sum\",\"score\":\"sum\"}).sort(col('count(author)').desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odd that the preceding authors have no down but this is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+----------+--------+\n",
      "|          author|sum(score)|sum(downs)|sum(ups)|\n",
      "+----------------+----------+----------+--------+\n",
      "|    ItWillBeMine|     -6839|         0|   -6839|\n",
      "|        blaghart|     -4233|         0|   -4233|\n",
      "|       Shanondoa|     -3555|         0|   -3555|\n",
      "|   bad_driverman|     -3053|         0|   -3053|\n",
      "|      RSneedsEoC|     -2192|         0|   -2192|\n",
      "|   b00gymonster1|     -2050|         0|   -2050|\n",
      "|      frankenham|     -2024|         0|   -2024|\n",
      "|   SaddharKadham|     -1485|         0|   -1485|\n",
      "|letters_numbers-|     -1412|         0|   -1412|\n",
      "|     djroomba322|     -1392|         0|   -1392|\n",
      "+----------------+----------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if runEDA:\n",
    "    # Show authors with the lowest scores\n",
    "    df.groupby('author').agg({\"score\":\"sum\",\"ups\":\"sum\",\"downs\":\"sum\"}).sort(col('sum(score)').asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|scoreSentiment|count(scoreSentiment)|\n",
      "+--------------+---------------------+\n",
      "|             0|               394008|\n",
      "|             1|              8432020|\n",
      "+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if runEDA:\n",
    "    # Get a summary of score sentiment by label\n",
    "    sentDF=df.groupby('scoreSentiment').agg({\"scoreSentiment\":\"count\"}).sort(col('scoreSentiment').asc())\n",
    "    sentDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if runEDA:\n",
    "    # convert to pandas DF\n",
    "    sentDF_pandas=sentDF.toPandas()\n",
    "    sentDF_pandas['scoreSentiment'] = sentDF_pandas['scoreSentiment'].astype(str) \n",
    "    sentDF_pandas['scoreSentiment']=sentDF_pandas['scoreSentiment'].replace(['0'],'Negative')\n",
    "    sentDF_pandas['scoreSentiment']=sentDF_pandas['scoreSentiment'].replace(['1'],'Positive')\n",
    "    sentDF_pandas.plot(kind='bar', x='scoreSentiment', y='count(scoreSentiment)', colormap='winter_r')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #sentDF.registerTempTable(\"sentiment_table\")\n",
    "    #display(SQLContext(sc).sql(\"select * from sentiment_table\"))\n",
    "    #sentDF.select('scoreSentiment').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runEDA:\n",
    "    # Generate historgram of body word count\n",
    "    # There are a small (relative) number of long comments but most are under 100 words\n",
    "    maxWords=100\n",
    "    bwcDF=df.filter(col('bodyWordCount')<=maxWords).select('bodyWordCount')\n",
    "    bwcDF_pandas=bwcDF.toPandas()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    numBins = 50\n",
    "    ax.hist(bwcDF_pandas,numBins,color='green',alpha=0.8)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Predict Sentiment from body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               words|                  tf|\n",
      "+--------------------+--------------------+\n",
      "|              [くそ]|(262144,[85691],[...|\n",
      "|[gg, this, one's,...|(262144,[5674,905...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create TF (Term Frequency) feature\n",
    "tok = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "htf = HashingTF(inputCol=\"words\", outputCol=\"tf\")  # numFeatures will be a hyper-parameter  \n",
    "\n",
    "#testing\n",
    "tmpDF=tok.transform(df)\n",
    "tmpDF=htf.transform(tmpDF)\n",
    "tmpDF.select('words','tf').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntmpDF=rva.transform(df)\\nmodel=w2v.fit(tmpDF)\\ntmpDF=model.transform(tmpDF)\\ntmpDF.show(2)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create w2v (word to vec) feature\n",
    "\n",
    "# the comment string needs to be turned into a vector for w2v to work\n",
    "# unfortunately, VectorAssember does not work on string so we need a UDF\n",
    "\n",
    "# Create UDF (note: split(anything,0) simply means don't split)\n",
    "str_to_vec=spark.udf.register(\"str_to_vec\",\n",
    "                             lambda row:row.split(\"#\",0),\n",
    "                             ArrayType(StringType()))\n",
    "\n",
    "# set up the tranformation\n",
    "rva=SQLTransformer(statement=\"SELECT *, str_to_vec(body) bodyVec FROM __THIS__\")\n",
    "\n",
    "w2v = Word2Vec(inputCol='bodyVec', outputCol='w2v')  # not setting minCount \n",
    "\n",
    "# testing\n",
    "\"\"\"\n",
    "tmpDF=rva.transform(df)\n",
    "model=w2v.fit(tmpDF)\n",
    "tmpDF=model.transform(tmpDF)\n",
    "tmpDF.show(2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble predictors\n",
    "va=VectorAssembler(inputCols=['tf','w2v','bwFlag','bodyWordCount'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the regression model; regParam & elasticNetParam will be hyper-parameters\n",
    "# CrossVal currently requires the labelCol to be precisely called 'label'\n",
    "#lr = LogisticRegression(labelCol='scoreSentiment',maxIter=10)\n",
    "lr = LogisticRegression(labelCol='label',maxIter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "#pipeline=Pipeline(stages=[bkt,tok,htf,rva,w2v,va,lr])  # took out bkt since this is pre-EDA\n",
    "pipeline=Pipeline(stages=[tok,htf,rva,w2v,va,lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up hyperparameter tuning & Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparamGrid = ParamGridBuilder()     .addGrid(htf.numFeatures, [200])     .addGrid(lr.regParam, [0.3])     .addGrid(lr.elasticNetParam, [0.5])     .build()\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the parameter grid\n",
    "\n",
    "\"\"\"\n",
    "# This version works and homes in on elasticNetParam=0\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(htf.numFeatures, [200]) \\\n",
    "    .addGrid(lr.regParam, [0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\"\"\"\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(htf.numFeatures, [200]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# This paramGrid for testing\n",
    "\"\"\"\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(htf.numFeatures, [200]) \\\n",
    "    .addGrid(lr.regParam, [0.3]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.5]) \\\n",
    "    .build()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('-'*30)\\n#print('paramGrid', paramGrid, '\\n')\\n#print('len(paramGrid): {}'.format(len(paramGrid)))\\nprint('-'*30)\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Too inspect paramGrid, uncomment next 4 lines\n",
    "\"\"\"\n",
    "print('-'*30)\n",
    "#print('paramGrid', paramGrid, '\\n')\n",
    "#print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "print('-'*30)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# Using the pipeline as the estimator slows things down but is necessary if tuning featurziers.  If not, set the \n",
    "# model specification as the estimator with estimator=lr (I think; though not sure if that means lr needs to be removed from pipeline)\n",
    "numFolds=5\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol='label'),\n",
    "                          numFolds=numFolds,\n",
    "                          collectSubModels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the benchmark model (only based on badwords & word count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up featuresRF\n",
    "vaRF=VectorAssembler(inputCols=['bwFlag','bodyWordCount'],outputCol='featuresRF')\n",
    "\n",
    "# Define the RF classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"featuresRF\", numTrees=10)\n",
    "\n",
    "# Create the pipeline\n",
    "rfPipeline = Pipeline(stages=[vaRF,rf])\n",
    "\n",
    "# Fit the model using the training data\n",
    "rfModel = rfPipeline.fit(trainDF)\n",
    "\n",
    "# save the rf model with a timestamp\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "rfModel.save(\"rfModel\"+timestr)\n",
    "\n",
    "print(\"RF model fitting complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF prediction complete\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictionRF = rfModel.transform(testDF)\n",
    "print (\"RF prediction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfEvaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfAccuracy = rfEvaluator.evaluate(predictionRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9552675049687026\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy for RF:\", rfAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine parallelism \n",
    "# This resource: see https://databricks.com/session/model-parallelism-in-spark-ml-cross-validation\n",
    "# says that best practice is parallelism = (# cores)/(# partitions) but generally not more than 10\n",
    "numPartitions=trainDF.rdd.getNumPartitions()\n",
    "numCores=sc.defaultParallelism\n",
    "parallelism=int(round(numCores/numPartitions,0))\n",
    "# also see https://stackoverflow.com/questions/42171499/get-current-number-of-partitions-of-a-dataframe\n",
    "\n",
    "# constrain to between 1 and 10\n",
    "if parallelism<1:\n",
    "    parallelism=1\n",
    "elif parallelism > 10:\n",
    "    parallelism=10\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "    \n",
    "\"\"\"\n",
    "# Another thing we can do is treat cores as fixed and repartition to get a target parallelism\n",
    "# while avoiding memory issues that occur when != cores/partitions\n",
    "# in the future: verify cores/partitions is correct; might want to do something to avoid having\n",
    "# too few partitions\n",
    "parallelism=2\n",
    "targetNumPartitions=int(round(numCores/parallelism,0))\n",
    "if (targetNumPartitions>=1):\n",
    "    if (targetNumPartitions<numPartitions):\n",
    "        trainDF = trainDF.coalesce(targetNumPartitions) # no shuffling but can only be used for decreasing numPartitions\n",
    "    else: \n",
    "        trainDF = trainDF.repartition(targetNumPartitions)  # this involves shuffling to less efficient\n",
    "\"\"\" \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# However, elsewhere, you typically see that partitions should be 2x to 4x the number of cores!\n",
    "# So, we could just override (note: 4 yielded memory errors)\n",
    "if overrideParallelism:\n",
    "    parallelism=overrideParallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out parallelism\n",
    "parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache trainDF to speed up cross validation; we could use .select(colnames...) to use less memory\n",
    "# Cache & persist failed with 96GB and down to 50/50 train test split\n",
    "# yeah! worked with 25/25/50 train/test/holdout split with 96GB allocated!!!\n",
    "if persistTrainDF:\n",
    "    #trainDF=trainDF.cache()\n",
    "    trainDF=trainDF.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    trainDF.count()  # call count to actually cache the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 3352.6148397922516\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "if len(loadCVmodel)==0:\n",
    "    # Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    if parallelism<=1:\n",
    "        cvModel = crossval.fit(trainDF) # train models (no parallelism)\n",
    "    else:    \n",
    "        cvModel = crossval.setParallelism(parallelism).fit(trainDF) # train models in parallel\n",
    "    print(\"train time:\", time.time() - t0)\n",
    "    print('-'*30)\n",
    "    # Took 3580 secs (~1hr) to run single params set with 50/50 split, 5 fold on 8 cores with 32 GB memmory & no parallelism & no cache/persist\n",
    "    # 10/10/90 train/test/holdout without parallelism took 3352 secs for 6 model variations (10 mins per model)\n",
    "    \n",
    "    # save the model with a timestamp\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    cvModel.save(\"lrModel\"+timestr)\n",
    "    pipeline.save(\"lrPipeline\"+timestr)\n",
    "else: \n",
    "    # Load the model and the pipeline  (should these be preceded by \"val\")\n",
    "    cvModel = CrossValidatorModel.load(loadCVmodel)\n",
    "    #val sameModel = PipelineModel.load(\"/path-to-my-pipeline/spark-log-reg-transfer-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cache\n",
    "if persistTrainDF:\n",
    "    trainDF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_4446ecd1f38f', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_4446ecd1f38f', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages[-1].extractParamMap()\n",
    "# best model has the following:\n",
    "# elasticNetParam = 0\n",
    "# regParam = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the necessary evaluators\n",
    "evaluator=BinaryClassificationEvaluator(labelCol='label')\n",
    "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predict_train=cvModel.transform(trainDF)\n",
    "predict_test=cvModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_test.show(3)\n",
    "# besides initial df cols and those created by pipeline, we ahve label, rawPrediction, probability, and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9549860734338707\n",
      "Test Accuracy: 0.9552607024095798\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy:\", mcEvaluator.evaluate(predict_train))\n",
    "print(\"Test Accuracy:\", mcEvaluator.evaluate(predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precision, recall, F1 score\n",
    "Source: https://stackoverflow.com/questions/60772315/how-to-evaluate-a-classifier-with-pyspark-2-4-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision is 0.9214841866897692\n",
      "Test recall is 0.9552607024095798\n",
      "Test f1 is 0.9334140348304236\n"
     ]
    }
   ],
   "source": [
    "weightedPrecision = mcEvaluator.evaluate(predict_test, {mcEvaluator.metricName: \"weightedPrecision\"})\n",
    "print(\"Test precision is {}\".format(weightedPrecision))\n",
    "\n",
    "weightedRecall = mcEvaluator.evaluate(predict_test, {mcEvaluator.metricName: \"weightedRecall\"})\n",
    "print(\"Test recall is {}\".format(weightedRecall))\n",
    "\n",
    "f1 = mcEvaluator.evaluate(predict_test, {mcEvaluator.metricName: \"f1\"})\n",
    "print(\"Test f1 is {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train precision is 0.9232611846478553\n",
      "Train recall is 0.9549860734338705\n",
      "Train f1 is 0.9330062499313787\n"
     ]
    }
   ],
   "source": [
    "weightedPrecision = mcEvaluator.evaluate(predict_train, {mcEvaluator.metricName: \"weightedPrecision\"})\n",
    "print(\"Train precision is {}\".format(weightedPrecision))\n",
    "\n",
    "weightedRecall = mcEvaluator.evaluate(predict_train, {mcEvaluator.metricName: \"weightedRecall\"})\n",
    "print(\"Train recall is {}\".format(weightedRecall))\n",
    "\n",
    "f1 = mcEvaluator.evaluate(predict_train, {mcEvaluator.metricName: \"f1\"})\n",
    "print(\"Train f1 is {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Source: https://stackoverflow.com/questions/58404845/confusion-matrix-to-get-precsion-recall-f1score\n",
    "\n",
    "Confusion matrix references that may be helpful if the above does not work: \n",
    "\n",
    "https://gist.github.com/ispmarin/05feacd8be5e2901cf2b35453a148060\n",
    "\n",
    "https://shihaojran.com/distributed-machine-learning-using-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion matrix for test:\n",
      "[[2.00000e+00 3.94530e+04]\n",
      " [8.00000e+00 8.42558e+05]]\n"
     ]
    }
   ],
   "source": [
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "preds_and_labels = predict_test.select(['prediction','label']).withColumn('label', col('label').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(\"Confustion matrix for test:\")\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion matrix for train:\n",
      "[[2.00000e+00 3.97020e+04]\n",
      " [6.00000e+00 8.42417e+05]]\n"
     ]
    }
   ],
   "source": [
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "preds_and_labels = predict_train.select(['prediction','label']).withColumn('label', col('label').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(\"Confustion matrix for train:\")\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalTrain=evaluator.evaluate(predict_train)\n",
    "evalTest=evaluator.evaluate(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set after CV  is 0.6010364050742824\n",
      "The area under ROC for test set after CV  is 0.5955939049250416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evalTrain))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evalTest))\n",
    "\n",
    "# source: https://dhiraj-p-rai.medium.com/logistic-regression-in-spark-ml-8a95b5f5434c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My attempt to access submodels so don't need to fit again for sensitivity\n",
    "# There is very little documentation for this online.  subModels appears to be \n",
    "# a folds x models 2D list of pipelines. However, not sure how to extract a single model\n",
    "#cvModel.subModels\n",
    "#cvModel.subModels[1][1].stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our best model has elasticNetParam=0 and regParam=0.01.  So, we can do sensitivity analysis\n",
    "# by comparing to regParam=0.1 (we have this in subModels but I could not out how to access)\n",
    "paramGrid_sens = ParamGridBuilder() \\\n",
    "    .addGrid(htf.numFeatures, [200]) \\\n",
    "    .addGrid(lr.regParam, [0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# Using the pipeline as the estimator slows things down but is necessary if tuning featurziers.  If not, set the \n",
    "# model specification as the estimator with estimator=lr (I think; though not sure if that means lr needs to be removed from pipeline)\n",
    "numFolds=5\n",
    "crossval_sens = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid_sens,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol='label'),\n",
    "                          numFolds=numFolds,\n",
    "                          collectSubModels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(loadCVmodelSens)==0:\n",
    "    cvModel_sens = crossval_sens.fit(trainDF) # train models (no parallelism)\n",
    "    print (\"Sensitivity fit complete\")\n",
    "else:\n",
    "    cvModel_sens = CrossValidatorModel.load(loadCVmodelSens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sensitivity model if it was just fitted\n",
    "if len(loadCVmodelSens)==0:\n",
    "    # save the model with a timestamp\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    cvModel_sens.save(\"lrModelSens\"+timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity prediction generation complete\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predict_train_sens=cvModel_sens.transform(trainDF)\n",
    "predict_test_sens=cvModel_sens.transform(testDF)\n",
    "print (\"Sensitivity prediction generation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity evaluate complete\n"
     ]
    }
   ],
   "source": [
    "evaluator=BinaryClassificationEvaluator(labelCol='label')\n",
    "evalTrain=evaluator.evaluate(predict_train_sens)\n",
    "evalTest=evaluator.evaluate(predict_test_sens)\n",
    "print (\"Sensitivity evaluate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set after CV  is 0.5997333917195864\n",
      "The area under ROC for test set after CV  is 0.5950056297766232\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evalTrain))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evalTest))\n",
    "\n",
    "# source: https://dhiraj-p-rai.medium.com/logistic-regression-in-spark-ml-8a95b5f5434c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save notebook as PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jeremey_code.ipynb to pdf\n",
      "[NbConvertApp] Writing 123567 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 112633 bytes to /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jeremey_code.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jeremey_code_BKUP202107300829.ipynb to pdf\n",
      "[NbConvertApp] Writing 128667 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 119269 bytes to /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jeremey_code_BKUP202107300829.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jeremey_test.ipynb to pdf\n",
      "[NbConvertApp] Writing 23345 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 23443 bytes to /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jeremey_test.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jordan_features.ipynb to pdf\n",
      "[NbConvertApp] Writing 32057 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 32123 bytes to /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jordan_features.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jordan_subset.ipynb to pdf\n",
      "[NbConvertApp] Writing 41995 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 42636 bytes to /sfs/qumulo/qhome/jdd5dw/ds5110-project/Jordan_subset.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/jdd5dw/ds5110-project/ben.ipynb to pdf\n",
      "[NbConvertApp] Writing 55748 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 48364 bytes to /sfs/qumulo/qhome/jdd5dw/ds5110-project/ben.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/jdd5dw/ds5110-project/test_file.ipynb to pdf\n",
      "[NbConvertApp] Writing 26544 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] CRITICAL | xelatex failed: ['xelatex', 'notebook.tex', '-quiet']\n",
      "This is XeTeX, Version 3.14159265-2.6-0.99999 (TeX Live 2019/dev/Debian) (preloaded format=xelatex)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n",
      "(./notebook.tex\n",
      "LaTeX2e <2018-12-01>\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls\n",
      "Document Class: article 2018/09/03 v1.4i Standard LaTeX document class\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/size11.clo))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcolorbox.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-lists.t\n",
      "ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.def\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ms/everyshi.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/pgf.revision.tex)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphicx.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics-def/xetex.def)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.code.t\n",
      "ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.def\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfmx.def\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-pdf.de\n",
      "f))))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath.code.\n",
      "tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol.code.\n",
      "tex)) (/usr/share/texlive/texmf-dist/tex/latex/xcolor/xcolor.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/color.cfg))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basic.code\n",
      ".tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trigonomet\n",
      "ric.code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.random.cod\n",
      "e.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comparison\n",
      ".code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base.code.\n",
      "tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.round.code\n",
      ".tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc.code.\n",
      "tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.integerari\n",
      "thmetics.code.tex)))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.code.te\n",
      "x)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathconstruct.\n",
      "code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusage.code\n",
      ".tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.code.te\n",
      "x)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphicstate.c\n",
      "ode.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransformation\n",
      "s.code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.code.tex\n",
      ")\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.code.t\n",
      "ex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathprocessing\n",
      ".code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.code.te\n",
      "x)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.code.tex\n",
      ")\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.code.tex\n",
      "\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal.code.\n",
      "tex))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.code.te\n",
      "x)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransparency.c\n",
      "ode.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns.code.\n",
      "tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code.tex))\n",
      ")\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.code.tex\n",
      ") (/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code.tex\n",
      ")\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-0-65\n",
      ".sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-1-18\n",
      ".sty)) (/usr/share/texlive/texmf-dist/tex/latex/tools/verbatim.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/environ/environ.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/trimspaces/trimspaces.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/etoolbox/etoolbox.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcbbreakable.code.tex\n",
      "Library (tcolorbox): 'tcbbreakable.code.tex' version '4.15'\n",
      ")) (/usr/share/texlive/texmf-dist/tex/latex/parskip/parskip.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/kvoptions.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ltxcmds.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/kvsetkeys.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/infwarerr.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/etexcmds.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifluatex.sty)))))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/iftex/iftex.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/l3packages/xparse/xparse.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/l3kernel/expl3.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/l3kernel/expl3-code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/l3kernel/l3xdvipdfmx.def)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/tuenc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec.cfg)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/caption/caption.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/caption/caption3.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/float/float.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/tools/enumerate.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifpdf.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifvtex.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty\n",
      "For additional information on amsmath, use the `?' option.\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/textcomp.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/ts1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/upquote/upquote.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/eurosym/eurosym.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ucs/ucs.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ucs/data/uni-global.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/fancyvrb/fancyvrb.sty\n",
      "Style option: `fancyvrb' v3.2a <2019/01/15> (tvz))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/grffile.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/pdftexcmds.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjustbox.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/xkeyval/xkeyval.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkeyval.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkvutils.tex)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjcalc.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/trimclip.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/collectbox/collectbox.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/tc-xetex.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ifoddpage/ifoddpage.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/varwidth/varwidth.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-hyperref.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-generic.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/auxhook.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/pd1enc.def)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/hyperref.cfg)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/url/url.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hxetex.def\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/puenc.def)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/stringenc.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/rerunfilecheck.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/titling/titling.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/tools/longtable.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/booktabs/booktabs.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/enumitem/enumitem.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/ulem/ulem.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/mathrsfs.sty)\n",
      "No file notebook.aux.\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/ts1cmr.fd)\n",
      "ABD: EveryShipout initializing macros\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/caption/ltcaption.sty)\n",
      "*geometry* driver: auto-detecting\n",
      "*geometry* detected driver: xetex\n",
      "*geometry* verbose mode - [ preamble ] result:\n",
      "* driver: xetex\n",
      "* paper: <default>\n",
      "* layout: <same size as paper>\n",
      "* layoutoffset:(h,v)=(0.0pt,0.0pt)\n",
      "* modes: \n",
      "* h-part:(L,W,R)=(72.26999pt, 469.75502pt, 72.26999pt)\n",
      "* v-part:(T,H,B)=(72.26999pt, 650.43001pt, 72.26999pt)\n",
      "* \\paperwidth=614.295pt\n",
      "* \\paperheight=794.96999pt\n",
      "* \\textwidth=469.75502pt\n",
      "* \\textheight=650.43001pt\n",
      "* \\oddsidemargin=0.0pt\n",
      "* \\evensidemargin=0.0pt\n",
      "* \\topmargin=-37.0pt\n",
      "* \\headheight=12.0pt\n",
      "* \\headsep=25.0pt\n",
      "* \\topskip=11.0pt\n",
      "* \\footskip=30.0pt\n",
      "* \\marginparwidth=59.0pt\n",
      "* \\marginparsep=10.0pt\n",
      "* \\columnsep=10.0pt\n",
      "* \\skip\\footins=10.0pt plus 4.0pt minus 2.0pt\n",
      "* \\hoffset=0.0pt\n",
      "* \\voffset=0.0pt\n",
      "* \\mag=1000\n",
      "* \\@twocolumnfalse\n",
      "* \\@twosidefalse\n",
      "* \\@mparswitchfalse\n",
      "* \\@reversemarginfalse\n",
      "* (1in=72.27pt=25.4mm, 1cm=28.453pt)\n",
      "\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ucs/ucsencs.def)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/nameref.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/gettitlestring.sty))\n",
      "\n",
      "Package hyperref Warning: Rerun to get /PageLabels entry.\n",
      "\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsa.fd)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsb.fd)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/ursfs.fd)\n",
      "\n",
      "LaTeX Warning: No \\author given.\n",
      "\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/se-ascii-print.def)\n",
      "\n",
      "LaTeX Warning: File `attachment:27d67016-1a87-45ec-acc8-cf6a498607c6.png' not f\n",
      "ound on input line 398.\n",
      "\n",
      "! Unable to load picture or PDF file 'attachment:27d67016-1a87-45ec-acc8-cf6a49\n",
      "8607c6.png'.\n",
      "<to be read again> \n",
      "                   }\n",
      "l.398 ...27d67016-1a87-45ec-acc8-cf6a498607c6.png}\n",
      "                                                  \n",
      "? \n",
      "! Emergency stop.\n",
      "<to be read again> \n",
      "                   }\n",
      "l.398 ...27d67016-1a87-45ec-acc8-cf6a498607c6.png}\n",
      "                                                  \n",
      "No pages of output.\n",
      "Transcript written on notebook.log.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/jupyter-nbconvert\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/jupyter_core/application.py\", line 254, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/nbconvertapp.py\", line 350, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/nbconvertapp.py\", line 524, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/nbconvertapp.py\", line 489, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(notebook_filename, resources, input_buffer=input_buffer)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/nbconvertapp.py\", line 418, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/exporters/exporter.py\", line 181, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/exporters/exporter.py\", line 199, in from_file\n",
      "    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, **kw)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/exporters/pdf.py\", line 183, in from_notebook_node\n",
      "    self.run_latex(tex_file)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/exporters/pdf.py\", line 154, in run_latex\n",
      "    self.latex_count, log_error, raise_on_failure)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/nbconvert/exporters/pdf.py\", line 143, in run_command\n",
      "    command=command, output=out))\n",
      "nbconvert.exporters.pdf.LatexFailed: PDF creating failed, captured latex output:\n",
      "Failed to run \"['xelatex', 'notebook.tex', '-quiet']\" command:\n",
      "This is XeTeX, Version 3.14159265-2.6-0.99999 (TeX Live 2019/dev/Debian) (preloaded format=xelatex)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n",
      "(./notebook.tex\n",
      "LaTeX2e <2018-12-01>\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls\n",
      "Document Class: article 2018/09/03 v1.4i Standard LaTeX document class\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/size11.clo))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcolorbox.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-lists.t\n",
      "ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.def\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ms/everyshi.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/pgf.revision.tex)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphicx.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics-def/xetex.def)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.code.t\n",
      "ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.def\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfmx.def\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-pdf.de\n",
      "f))))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath.code.\n",
      "tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol.code.\n",
      "tex)) (/usr/share/texlive/texmf-dist/tex/latex/xcolor/xcolor.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/color.cfg))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basic.code\n",
      ".tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trigonomet\n",
      "ric.code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.random.cod\n",
      "e.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comparison\n",
      ".code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base.code.\n",
      "tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.round.code\n",
      ".tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc.code.\n",
      "tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.integerari\n",
      "thmetics.code.tex)))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.code.te\n",
      "x)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathconstruct.\n",
      "code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusage.code\n",
      ".tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.code.te\n",
      "x)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphicstate.c\n",
      "ode.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransformation\n",
      "s.code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.code.tex\n",
      ")\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.code.t\n",
      "ex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathprocessing\n",
      ".code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.code.te\n",
      "x)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.code.tex\n",
      ")\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.code.tex\n",
      "\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal.code.\n",
      "tex))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.code.te\n",
      "x)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransparency.c\n",
      "ode.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns.code.\n",
      "tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code.tex))\n",
      ")\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.code.tex\n",
      ") (/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code.tex\n",
      ")\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-0-65\n",
      ".sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-1-18\n",
      ".sty)) (/usr/share/texlive/texmf-dist/tex/latex/tools/verbatim.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/environ/environ.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/trimspaces/trimspaces.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/etoolbox/etoolbox.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcbbreakable.code.tex\n",
      "Library (tcolorbox): 'tcbbreakable.code.tex' version '4.15'\n",
      ")) (/usr/share/texlive/texmf-dist/tex/latex/parskip/parskip.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/kvoptions.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ltxcmds.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/kvsetkeys.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/infwarerr.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/etexcmds.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifluatex.sty)))))\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/iftex/iftex.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/l3packages/xparse/xparse.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/l3kernel/expl3.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/l3kernel/expl3-code.tex)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/l3kernel/l3xdvipdfmx.def)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/tuenc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec.cfg)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/caption/caption.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/caption/caption3.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/float/float.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/tools/enumerate.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifpdf.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifvtex.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty\n",
      "For additional information on amsmath, use the `?' option.\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/textcomp.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/ts1enc.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/upquote/upquote.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/eurosym/eurosym.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ucs/ucs.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ucs/data/uni-global.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/fancyvrb/fancyvrb.sty\n",
      "Style option: `fancyvrb' v3.2a <2019/01/15> (tvz))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/grffile.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/pdftexcmds.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjustbox.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/xkeyval/xkeyval.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkeyval.tex\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkvutils.tex)))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjcalc.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/trimclip.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/collectbox/collectbox.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/tc-xetex.def))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ifoddpage/ifoddpage.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/varwidth/varwidth.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-hyperref.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-generic.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/auxhook.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/pd1enc.def)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/hyperref.cfg)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/url/url.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hxetex.def\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/puenc.def)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/stringenc.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/rerunfilecheck.sty))\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/titling/titling.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/tools/longtable.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/booktabs/booktabs.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/enumitem/enumitem.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/ulem/ulem.sty)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/mathrsfs.sty)\n",
      "No file notebook.aux.\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/base/ts1cmr.fd)\n",
      "ABD: EveryShipout initializing macros\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/caption/ltcaption.sty)\n",
      "*geometry* driver: auto-detecting\n",
      "*geometry* detected driver: xetex\n",
      "*geometry* verbose mode - [ preamble ] result:\n",
      "* driver: xetex\n",
      "* paper: <default>\n",
      "* layout: <same size as paper>\n",
      "* layoutoffset:(h,v)=(0.0pt,0.0pt)\n",
      "* modes: \n",
      "* h-part:(L,W,R)=(72.26999pt, 469.75502pt, 72.26999pt)\n",
      "* v-part:(T,H,B)=(72.26999pt, 650.43001pt, 72.26999pt)\n",
      "* \\paperwidth=614.295pt\n",
      "* \\paperheight=794.96999pt\n",
      "* \\textwidth=469.75502pt\n",
      "* \\textheight=650.43001pt\n",
      "* \\oddsidemargin=0.0pt\n",
      "* \\evensidemargin=0.0pt\n",
      "* \\topmargin=-37.0pt\n",
      "* \\headheight=12.0pt\n",
      "* \\headsep=25.0pt\n",
      "* \\topskip=11.0pt\n",
      "* \\footskip=30.0pt\n",
      "* \\marginparwidth=59.0pt\n",
      "* \\marginparsep=10.0pt\n",
      "* \\columnsep=10.0pt\n",
      "* \\skip\\footins=10.0pt plus 4.0pt minus 2.0pt\n",
      "* \\hoffset=0.0pt\n",
      "* \\voffset=0.0pt\n",
      "* \\mag=1000\n",
      "* \\@twocolumnfalse\n",
      "* \\@twosidefalse\n",
      "* \\@mparswitchfalse\n",
      "* \\@reversemarginfalse\n",
      "* (1in=72.27pt=25.4mm, 1cm=28.453pt)\n",
      "\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/ucs/ucsencs.def)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/hyperref/nameref.sty\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/gettitlestring.sty))\n",
      "\n",
      "Package hyperref Warning: Rerun to get /PageLabels entry.\n",
      "\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsa.fd)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsb.fd)\n",
      "(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/ursfs.fd)\n",
      "\n",
      "LaTeX Warning: No \\author given.\n",
      "\n",
      "(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/se-ascii-print.def)\n",
      "\n",
      "LaTeX Warning: File `attachment:27d67016-1a87-45ec-acc8-cf6a498607c6.png' not f\n",
      "ound on input line 398.\n",
      "\n",
      "! Unable to load picture or PDF file 'attachment:27d67016-1a87-45ec-acc8-cf6a49\n",
      "8607c6.png'.\n",
      "<to be read again> \n",
      "                   }\n",
      "l.398 ...27d67016-1a87-45ec-acc8-cf6a498607c6.png}\n",
      "                                                  \n",
      "? \n",
      "! Emergency stop.\n",
      "<to be read again> \n",
      "                   }\n",
      "l.398 ...27d67016-1a87-45ec-acc8-cf6a498607c6.png}\n",
      "                                                  \n",
      "No pages of output.\n",
      "Transcript written on notebook.log.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save notebook as PDF document\n",
    "!jupyter nbconvert --to pdf `pwd`/*.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
